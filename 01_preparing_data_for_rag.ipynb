{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b038b80a-601e-44ab-b236-556c4c3f8e62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Custom Installation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8ad5452-fbd0-4226-aadf-f1d026ecd457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "local_path = \"/Workspace/Users/scott.s@smartsoftus.com/rag/\"\n",
    "\n",
    "requirements_path = os.path.join(local_path, \"requirements.txt\")\n",
    "if not os.path.exists(requirements_path):\n",
    "    dbutils.fs.put(\"file:\" + requirements_path, \"\", True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6bd3bd-3b91-446c-a1e2-1bf1aea252bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r $requirements_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c1496b-c6f2-40cd-98fe-33291072ff1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d42f63a-891b-4845-a91c-db7fc0779af0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip freeze > requirements_freeze.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e818c373-7f8e-48c8-9372-0f1058816335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Use the existing Spark session\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(spark.version)\n",
    "print(type(spark))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b226dd6f-87a4-4be4-bff0-70387b24dc02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Prep for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c9a0e4-618b-44de-9740-69e33ee705a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reduce the arrow batch size as our PDF can be big in memory\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "333aa7d6-be19-40ea-9b22-23acd3986da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class paths:\n",
    "    datasets = \"/Volumes/arxiv_sample_articles/v01\"\n",
    "\n",
    "class DA:\n",
    "    paths = paths()\n",
    "    catalog_name = \"arxiv_sample_articles\"\n",
    "    schema_name = \"v01\"\n",
    "\n",
    "articles_path = f\"{DA.paths.datasets}/arxiv-articles/\"\n",
    "table_name = f\"{DA.catalog_name}.{DA.schema_name}.pdf_raw_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "229fb874-0d25-4327-885c-3190bf86c246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "      spark.read.format(\"binaryFile\")\n",
    "      .option(\"recursiveFileLookup\", \"true\")  \n",
    "      .load(articles_path)\n",
    "      )\n",
    "\n",
    "# save List of the files to table\n",
    "df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ea6a0cc-f0f3-46bb-8254-cccf11c61669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import fitz  \n",
    "\n",
    "def extract_doc_text(pdf_bytes: bytes) -> str:\n",
    "    text = \"\"\n",
    "    with fitz.open(stream=pdf_bytes, filetype=\"pdf\") as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8153f45b-8aaf-44d7-9ece-ea818eedba35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(f\"{articles_path.replace('dbfs:','/dbfs/')}2302.06476.pdf\", mode=\"rb\") as pdf:\n",
    "    doc = extract_doc_text(pdf.read())\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7f68ca-2f82-4d05-9b56-ff48a7ae6606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# from llama_index.langchain_helpers.text_splitter import SentenceSplitter\n",
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "# from llama_index import Document, set_global_tokenizer\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.utils import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Iterator\n",
    "from pyspark.sql.functions import col,udf,length,pandas_udf,explode\n",
    "from unstructured.partition.auto import partition\n",
    "\n",
    "@pandas_udf(\"array<string>\")\n",
    "def read_as_chunk(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    #set llama2 as tokenizer\n",
    "    set_global_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "    )\n",
    "    #Sentence splitter from llama_index to split on sentences\n",
    "    splitter = SentenceSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    def extract_and_split(b):\n",
    "        txt = extract_doc_text(b)\n",
    "        nodes = splitter.get_nodes_from_documents([Document(text=txt)])\n",
    "        return [n.text for n in nodes]\n",
    "        \n",
    "    for x in batch_iter:\n",
    "        yield x.apply(extract_and_split)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9dd755-a9fd-4269-a10f-57fa4fe184cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_chunks = (df\n",
    "                  .withColumn(\"content\", explode(read_as_chunk(\"content\")))\n",
    "                  .selectExpr('path as pdf_name','content')\n",
    ")\n",
    "display(df_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d67baf3b-8f50-4079-9fea-9088ac4ba6e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_preparing_data_for_rag",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
